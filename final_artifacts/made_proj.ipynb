{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "6OBVS4bvs1PP",
    "outputId": "7e1d3d6f-01db-4395-c7c1-5a4af9b043f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: bertopic in /usr/local/lib/python3.8/dist-packages (0.12.0)\n",
      "Requirement already satisfied: hdbscan>=0.8.28 in /usr/local/lib/python3.8/dist-packages (from bertopic) (0.8.29)\n",
      "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from bertopic) (0.5.3)\n",
      "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from bertopic) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.8/dist-packages (from bertopic) (1.0.2)\n",
      "Requirement already satisfied: pyyaml<6.0 in /usr/local/lib/python3.8/dist-packages (from bertopic) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.8/dist-packages (from bertopic) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.8/dist-packages (from bertopic) (1.21.6)\n",
      "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.8/dist-packages (from bertopic) (5.5.0)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.8/dist-packages (from bertopic) (1.3.5)\n",
      "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.8/dist-packages (from hdbscan>=0.8.28->bertopic) (0.29.32)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.8/dist-packages (from hdbscan>=0.8.28->bertopic) (1.9.3)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.8/dist-packages (from hdbscan>=0.8.28->bertopic) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1.5->bertopic) (2022.6)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from plotly>=4.7.0->bertopic) (8.1.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from plotly>=4.7.0->bertopic) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.25.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.4.1->bertopic) (1.12.1+cu113)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.1.97)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.11.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.13.1+cu113)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.4.1->bertopic) (3.7)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (4.1.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.0.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2022.6.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.13.2)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.8/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.8)\n",
      "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.8/dist-packages (from umap-learn>=0.5.0->bertopic) (0.56.4)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (4.13.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (57.4.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.39.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.49->umap-learn>=0.5.0->bertopic) (3.10.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (7.1.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2022.9.24)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.0.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (7.1.2)\n",
      "Found existing installation: scipy 1.9.3\n",
      "Uninstalling scipy-1.9.3:\n",
      "  Would remove:\n",
      "    /usr/local/lib/python3.8/dist-packages/scipy-1.9.3.dist-info/*\n",
      "    /usr/local/lib/python3.8/dist-packages/scipy.libs/libgfortran-040039e1.so.5.0.0\n",
      "    /usr/local/lib/python3.8/dist-packages/scipy.libs/libopenblasp-r0-41284840.3.18.so\n",
      "    /usr/local/lib/python3.8/dist-packages/scipy.libs/libquadmath-96973f99.so.0.0.0\n",
      "    /usr/local/lib/python3.8/dist-packages/scipy/*\n",
      "Proceed (y/n)? y\n",
      "  Successfully uninstalled scipy-1.9.3\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n",
      "Requirement already satisfied: numpy<1.26.0,>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from scipy) (1.21.6)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.9.3\n",
      "/usr/lib/python3.8/runpy.py:127: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install bertopic\n",
    "!pip uninstall scipy\n",
    "!pip install scipy\n",
    "!python -m nltk.downloader stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "VkMW79uQVNpy"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import spacy \n",
    "import random\n",
    "import pickle\n",
    "from bertopic import BERTopic\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "TOmWlTTdVUKx"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"all_data/data_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "j7qemN9YXXLP"
   },
   "outputs": [],
   "source": [
    "df = df[df.author_ids.apply(lambda x: not math.isnan(x) if not isinstance(x, str) else True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v9wg391rXYit",
    "outputId": "1802c564-bf9b-425a-b567-c63b75612b7f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(757871, 27)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dx8ztol5XZ5K",
    "outputId": "4d7c8646-ddca-4611-f73c-a37483c1a467"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2017., 2016., 2018., 2019., 2020., 2021.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.year.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E0oNUjeaXbXK",
    "outputId": "2670106d-0be4-4e02-98c1-10e99e338460"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split\n",
    "train, test = df[(df.year <= 2018)], df[(df.year > 2018) & (df.year < 2022)]\n",
    "train.shape[0] + test.shape[0] == df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KCqfXB9PXclD",
    "outputId": "0f875126-b9ca-4b86-bc40-07c0d3b590ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Процент наблюдений в тестовой выборке: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.68346328069025"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Процент наблюдений в тестовой выборке: \")\n",
    "test.shape[0] / (train.shape[0] + test.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "ba9oMMcLXeA9"
   },
   "outputs": [],
   "source": [
    "def get_dct_of_links(df):\n",
    "\n",
    "    authors = []\n",
    "    for indx, item in enumerate(df.author_ids.values):\n",
    "        authors.append(list(map(lambda x: x.strip(), item.split(\";\"))))\n",
    "\n",
    "    for lst in authors:\n",
    "        counter = Counter(lst)\n",
    "        for i in range(counter[\"\"]):\n",
    "            lst.remove(\"\")\n",
    "        \n",
    "    dct_of_links = defaultdict(set)\n",
    "    for lst in tqdm(authors):\n",
    "        if len(lst) != 1:\n",
    "            for i in range(len(lst)):\n",
    "                curr_lst = lst.copy()\n",
    "                curr_lst.remove(lst[i])\n",
    "                dct_of_links[lst[i]].update(curr_lst)\n",
    "    \n",
    "    return dct_of_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NEV79n79Xfvp",
    "outputId": "30eed75b-853e-4f76-ba55-16e037412fea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654168/654168 [00:02<00:00, 310251.31it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 103703/103703 [00:00<00:00, 464655.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# словари связей(автор: соавторы)\n",
    "train_dct_of_links, test_dct_of_links = get_dct_of_links(train), get_dct_of_links(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "583vHIAuXiQl",
    "outputId": "abb4ebac-d3b1-4341-e689-3b816c2dd38d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: \n",
      "[267, 275, 279, 279, 285, 289, 291, 293, 294, 301, 303, 309, 314, 320, 324, 351, 359, 363, 384, 386]\n",
      "test: \n",
      "[80, 80, 81, 81, 81, 85, 88, 93, 96, 98, 98, 99, 101, 101, 103, 104, 109, 110, 113, 140]\n"
     ]
    }
   ],
   "source": [
    "def print_top(dct, top):\n",
    "    out = []\n",
    "    for key, value in dct.items():\n",
    "        out.append(len(value))\n",
    "    print(sorted(out)[-top:])\n",
    "    \n",
    "# топ авторов по количеству соавторов\n",
    "print(\"train: \")\n",
    "print_top(train_dct_of_links, 20)\n",
    "print(\"test: \")\n",
    "print_top(test_dct_of_links, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['references', 'year', 'lang', 'volume', 'page_start', 'doi', 'title',\n",
       "       'issue', 'isbn', 'authors', 'abstract', 'pdf', 'issn', 'venue', 'fos',\n",
       "       'n_citation', 'page_end', 'keywords', 'url', 'author_ids',\n",
       "       'author_names', 'venue_id', 'venue_name', 'id', 'labels',\n",
       "       'used_in_bert_training', 'topics'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "KmHLzUDGXkAk",
    "outputId": "29c80b5e-1083-4b2f-a39b-8423d6b0304c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "757871it [4:39:55, 45.12it/s] \n"
     ]
    }
   ],
   "source": [
    "klusters_top = defaultdict(list)\n",
    "for item, label in tqdm(zip(df.author_ids.values, df.labels.values)):\n",
    "    for author in list(map(lambda x: x.strip(), item.split(\";\"))):\n",
    "        if author != \"\":\n",
    "            flag = False\n",
    "            for lst in klusters_top[label]:\n",
    "                if lst[0] == author:\n",
    "                    lst[1] += 1\n",
    "                    flag = True\n",
    "                    break\n",
    "            if flag is False:\n",
    "                klusters_top[label].append([author, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gJ6NrIVSQH0r",
    "outputId": "14873730-73b3-4f2e-fbd8-211d796343bb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2522/2522 [00:01<00:00, 2118.09it/s]\n"
     ]
    }
   ],
   "source": [
    "for key, value in tqdm(klusters_top.items()):\n",
    "    value.sort(key=lambda x: -x[1])\n",
    "    klusters_top[key] = np.array(klusters_top[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "L3dFURKEvYbR"
   },
   "outputs": [],
   "source": [
    "with open(\"klusters_top.pkl\", \"wb\") as file:\n",
    "    pickle.dump(klusters_top, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQMSqrhx4202"
   },
   "outputs": [],
   "source": [
    "class TopicModeling:\n",
    "\n",
    "    def __init__(self, path_to_model: str):\n",
    "        self.bert_model = BERTopic.load(path_to_model)\n",
    "        self.topic_dict = dict(zip(self.bert_model.get_topic_info()['Topic'], \n",
    "                                   self.bert_model.get_topic_info()['Name']))\n",
    "    \n",
    "    def text_preprocessing(self, text: str):\n",
    "        regex = re.compile('[A-Za-z]+')\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        mystopwords = stopwords.words('english') + ['paper', 'result', 'experiment', 'from', 'subject', \n",
    "                                                're', 'edu', 'use', 'data', 'method', 'based', \n",
    "                                                'new', 'approach', 'also','system', 'model', \n",
    "                                                'present', 'research', 'propose', 'base']\n",
    "        \n",
    "        text = ' '.join(regex.findall(text))\n",
    "        doc = nlp(text)\n",
    "        text = ' '.join([token.lemma_ for token in doc])\n",
    "        text = ' '.join([token for token in text.split() if not token in mystopwords])\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def score_text(self, text: str):\n",
    "        text = self.text_preprocessing(text)\n",
    "        topic, prob = self.bert_model.transform(text)\n",
    "        return self.topic_dict[topic[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9u_MB_IkXptr"
   },
   "outputs": [],
   "source": [
    "class Recommendation_system:\n",
    "    \n",
    "    def __init__(self, model_path, train_dct_of_links, klusters_top):\n",
    "        self.train_dct_of_links = train_dct_of_links\n",
    "        self.klusters_top = klusters_top\n",
    "        self.model = model_path\n",
    "        # with open(model_path, \"rb\") as f:\n",
    "        #     self.model = pickle.load(f)\n",
    "        self.model = model_path\n",
    "        self.top_authors = self.get_top_authors()\n",
    "\n",
    "    \n",
    "    def get_top_authors(self, top=1000):\n",
    "        authors_by_collaborators = [(author, len(collaborators)) for author, collaborators in self.train_dct_of_links.items()]\n",
    "        authors_by_collaborators.sort(key=lambda x: x[1])\n",
    "\n",
    "        out = [None] * top\n",
    "        for i in range(top):\n",
    "            out[i] = authors_by_collaborators[i][0]\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def delete_elements_from_set(st, count):\n",
    "        for _ in range(count):\n",
    "            st.pop()\n",
    "    \n",
    "\n",
    "    def add_recommendations_from_top(self, out, top):\n",
    "        indx = 0\n",
    "        while len(out) < top:\n",
    "            out.add(self.top_authors[indx])\n",
    "            indx += 1\n",
    "\n",
    "    def get_recommendations_on_articles(self, top, lst_of_articles):\n",
    "        out = set()\n",
    "        labels = [int(self.model.score_text(article).split(\"_\")[0]) for article in lst_of_articles]\n",
    "        labels = list(filter(lambda x: x in self.klusters_top, labels))\n",
    "        \n",
    "        if len(labels) != 0:\n",
    "            klusters_counter = Counter(labels).most_common()\n",
    "            klusters_count_for_recommendation = min(len(klusters_counter), 3)\n",
    "            articles_per_cluster = 1 if (top // klusters_count_for_recommendation) == 0 else (top // klusters_count_for_recommendation)\n",
    "\n",
    "\n",
    "            \n",
    "            for i in range(klusters_count_for_recommendation):\n",
    "                curr_kluster = klusters_counter[i][0]\n",
    "                out.update(self.klusters_top[curr_kluster][:articles_per_cluster, 0])\n",
    "                if len(out) >= top:\n",
    "                    self.delete_elements_from_set(st=out, \n",
    "                                                count=len(out) - top)\n",
    "                    return out\n",
    "                \n",
    "            \n",
    "            for i in range(klusters_count_for_recommendation):\n",
    "                curr_kluster = klusters_counter[i][0]\n",
    "                out.update(self.klusters_top[curr_kluster][articles_per_cluster:(articles_per_cluster * 2), 0])\n",
    "                if len(out) >= top:\n",
    "                    self.delete_elements_from_set(st=out, \n",
    "                                                count=len(out) - top)\n",
    "                    return out\n",
    "            \n",
    "\n",
    "            for i in range(klusters_count_for_recommendation, len(klusters_counter)):\n",
    "                curr_kluster = klusters_counter[i][0]\n",
    "                out.update(self.klusters_top[curr_kluster][articles_per_cluster:(articles_per_cluster * 2), 0])\n",
    "                if len(out) >= top:\n",
    "                    self.delete_elements_from_set(st=out, \n",
    "                                                count=len(out) - top)\n",
    "                    return out\n",
    "\n",
    "        self.add_recommendations_from_top(out, top)\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def get_recommendation(self, top=10, author_id=None, lst_of_articles=None):\n",
    "        if author_id is None:\n",
    "            if lst_of_articles is None:\n",
    "                return self.top_authors[-top:]\n",
    "\n",
    "            out = self.get_recommendations_on_articles(top=top,\n",
    "                                                       lst_of_articles=lst_of_articles)\n",
    "            \n",
    "            return out\n",
    "\n",
    "        elif author_id in self.train_dct_of_links:\n",
    "            all_recommendation = set()\n",
    "            first_layer = self.train_dct_of_links[author_id]\n",
    "            \n",
    "            for first_layer_item in first_layer:\n",
    "                for second_layer_item in self.train_dct_of_links[first_layer_item]:\n",
    "                    all_recommendation.add(second_layer_item)\n",
    "                    # если скорость работы будет позволять, можно добавлять всех, а потом, храня словарь \n",
    "                    # с количеством соавторов для каждого автора, добавлять их по популярности\n",
    "                    if len(all_recommendation) == top:\n",
    "                        return all_recommendation\n",
    "\n",
    "            self.add_recommendations_from_top(all_recommendation, top)\n",
    "            return all_recommendation\n",
    "\n",
    "        elif author_id not in self.train_dct_of_links:\n",
    "            if lst_of_articles is None:\n",
    "                return self.top_authors[-top:]\n",
    "            \n",
    "            out = self.get_recommendations_on_articles(top=top,\n",
    "                                                       lst_of_articles=lst_of_articles)\n",
    "            \n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSCILwbOLRd_"
   },
   "outputs": [],
   "source": [
    "# MODEL_PATH = \"drive/MyDrive/bert_model\"\n",
    "# model = BERTopic.load(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4M6KmWeL4ltR"
   },
   "outputs": [],
   "source": [
    "articles = [\"\"\"k-NN is a type of classification where the function is only approximated locally and all computation is deferred until function evaluation. Since this algorithm relies on distance for classification, if the features represent different physical units or come in vastly different scales then normalizing the training data can improve its accuracy dramatically.[3][4]\n",
    "Both for classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.[5]\n",
    "The neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.\n",
    "A peculiarity of the k-NN algorithm is that it is sensitive to the local structure of the data.\"\"\",\n",
    "\"\"\"For specific learning algorithms, it is possible to compute the gradient with respect to hyperparameters and then optimize the hyperparameters using gradient descent. The first usage of these techniques was focused on neural networks.[11] Since then, these methods have been extended to other models such as support vector machines[12] or logistic regression.[13]\n",
    "A different approach in order to obtain a gradient with respect to hyperparameters consists in differentiating the steps of an iterative optimization algorithm using automatic differentiation.[14][15][16][17] A more recent work along this direction uses the implicit function theorem to calculate hypergradients and proposes a stable approximation of the inverse Hessian. The method scales to millions of hyperparameters and requires constant memory.\n",
    "In a different approach,[18] a hypernetwork is trained to approximate the best response function. One of the advantages of this method is that it can handle discrete hyperparameters as well. Self-tuning networks[19] offer a memory efficient version of this approach by choosing a compact representation for the hypernetwork. More recently, Δ-STN[20] has improved this method further by a slight reparameterization of the hypernetwork which speeds up training. Δ-STN also yields a better approximation of the best-response Jacobian by linearizing the network in the weights, hence removing unnecessary nonlinear effects of large changes in the weights.\n",
    "Apart from hypernetwork approaches, gradient-based methods can be used to optimize discrete hyperparameters also by adopting a continuous relaxation of the parameters.[21] Such methods have been extensively used for the optimization of architecture hyperparameters in neural architecture search.\n",
    "\"\"\",\n",
    "\"\"\"Spectral methods and finite element methods are closely related and built on the same ideas; the main difference between them is that spectral methods use basis functions that are generally nonzero over the whole domain, while finite element methods use basis functions that are nonzero only on small subdomains (compact support). Consequently, spectral methods connect variables globally while finite elements do so locally. Partially for this reason, spectral methods have excellent error properties, with the so-called \"exponential convergence\" being the fastest possible, when the solution is smooth. However, there are no known three-dimensional single domain spectral shock capturing results (shock waves are not smooth).[1] In the finite element community, a method where the degree of the elements is very high or increases as the grid parameter h increases is sometimes called a spectral element method.\n",
    "Spectral methods can be used to solve differential equations (PDEs, ODEs, eigenvalue, etc) and optimization problems. When applying spectral methods to time-dependent PDEs, the solution is typically written as a sum of basis functions with time-dependent coefficients; substituting this in the PDE yields a system of ODEs in the coefficients which can be solved using any numerical method for ODEs. Eigenvalue problems for ODEs are similarly converted to matrix eigenvalue problems[citation needed].\n",
    "Spectral methods were developed in a long series of papers by Steven Orszag starting in 1969 including, but not limited to, Fourier series methods for periodic geometry problems, polynomial spectral methods for finite and unbounded geometry problems, pseudospectral methods for highly nonlinear problems, and spectral iteration methods for fast solution of steady-state problems. The implementation of the spectral method is normally accomplished either with collocation or a Galerkin or a Tau approach . For very small problems, the spectral method is unique that solutions may be written out symbolically, yielding a practical alternative to series solutions for differential equations.\n",
    "Spectral methods can be computationally less expensive and easier to implement than finite element methods; they shine best when high accuracy is sought in simple domains with smooth solutions. However, because of their global nature, the matrices associated with step computation are dense and computational efficiency will quickly suffer when there are many degrees of freedom (with some exceptions, for example if matrix applications can be written as Fourier transforms). For larger problems and nonsmooth solutions, finite elements will generally work better due to sparse matrices and better modelling of discontinuities and sharp bends.\n",
    "\"\"\",\n",
    "\"\"\"Calculus, originally called infinitesimal calculus or \"the calculus of infinitesimals\", is the mathematical study of continuous change, in the same way that geometry is the study of shape, and algebra is the study of generalizations of arithmetic operations.\n",
    "It has two major branches, differential calculus and integral calculus; the former concerns instantaneous rates of change, and the slopes of curves, while the latter concerns accumulation of quantities, and areas under or between curves. These two branches are related to each other by the fundamental theorem of calculus, and they make use of the fundamental notions of convergence of infinite sequences and infinite series to a well-defined limit.[1]\n",
    "Infinitesimal calculus was developed independently in the late 17th century by Isaac Newton and Gottfried Wilhelm Leibniz.[2][3] Later work, including codifying the idea of limits, put these developments on a more solid conceptual footing. Today, calculus has widespread uses in science, engineering, and social science.[4]\n",
    "\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G4CgVmJvLXi7"
   },
   "outputs": [],
   "source": [
    "model_path=\"drive/MyDrive/topic_modeling_pipeline.pkl\"\n",
    "with open(model_path, \"rb\") as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eWs6keapehv1",
    "outputId": "eb812dd8-3b85-4c76-ebed-53db6437b34a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125497/125497 [00:01<00:00, 79876.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.79 s, sys: 8.79 ms, total: 1.8 s\n",
      "Wall time: 1.83 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "RecSys = Recommendation_system(train_dct_of_links=train_dct_of_links,\n",
    "                               model_path=model,\n",
    "                               klusters_top=klusters_top)\n",
    "\n",
    "for author_id in tqdm(test_dct_of_links):\n",
    "    value = random.randint(1, 100)\n",
    "    recommendations = RecSys.get_recommendation(top=value,\n",
    "                                                author_id=None,\n",
    "                                                lst_of_articles=list())\n",
    "    if len(recommendations) != value:\n",
    "        print(author_id, len(recommendations), value)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
